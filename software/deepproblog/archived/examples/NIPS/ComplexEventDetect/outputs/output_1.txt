(DPL) nothing@nothing:~/Code/ece209as_project/software/deepproblog/archived/examples/NIPS/ComplexEventDetect$ python run.py 
Training for 1 epochs (47933 iterations).
/home/nothing/miniconda3/envs/DPL/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Accuracy 0.1683991683991684
Epoch 1
Iteration:  100         Average Loss:  1.9606356538306846
Iteration:  200         Average Loss:  1.7642897919836196
Iteration:  300         Average Loss:  1.7658137923577535
Iteration:  400         Average Loss:  1.7019012269642786
Iteration:  500         Average Loss:  1.6393289680116248
Iteration:  600         Average Loss:  1.6499427497252923
Iteration:  700         Average Loss:  1.6667968364384438
Iteration:  800         Average Loss:  1.6351339574904327
Iteration:  900         Average Loss:  1.553249355458947
Iteration:  1000        Average Loss:  1.5263197013285599
Iteration:  1100        Average Loss:  1.6492032445665887
Iteration:  1200        Average Loss:  1.6368887202889784
Iteration:  1300        Average Loss:  1.575556530986161
Iteration:  1400        Average Loss:  1.5683710362338825
Iteration:  1500        Average Loss:  1.6004566995610192
Iteration:  1600        Average Loss:  1.6005825434661938
Iteration:  1700        Average Loss:  1.5710323145630787
Iteration:  1800        Average Loss:  1.8583865971519953
Iteration:  1900        Average Loss:  1.4512959871864988
Iteration:  2000        Average Loss:  1.5215470616612192
Iteration:  2100        Average Loss:  1.4824517801113675
Iteration:  2200        Average Loss:  1.6398462802756937
Iteration:  2300        Average Loss:  1.5143873867707212
Iteration:  2400        Average Loss:  1.4609026691477682
Iteration:  2500        Average Loss:  1.5061963411751387
Iteration:  2600        Average Loss:  1.475820459558844
Iteration:  2700        Average Loss:  1.4819427466041446
Iteration:  2800        Average Loss:  1.5292415942783668
Iteration:  2900        Average Loss:  1.5323291142182578
Iteration:  3000        Average Loss:  1.5339758524962406
Iteration:  3100        Average Loss:  1.508716049963689
Iteration:  3200        Average Loss:  1.427511261783006
Iteration:  3300        Average Loss:  1.5317931468796866
Iteration:  3400        Average Loss:  1.7174755578354646
Iteration:  3500        Average Loss:  1.4883688091313467
Iteration:  3600        Average Loss:  1.4891157649648918
Iteration:  3700        Average Loss:  1.6266203292235375
Iteration:  3800        Average Loss:  1.6340716699824704
Iteration:  3900        Average Loss:  1.555551028498294
Iteration:  4000        Average Loss:  1.6854993041083584
Iteration:  4100        Average Loss:  1.6339148636785052
Iteration:  4200        Average Loss:  1.6301608617002123
Iteration:  4300        Average Loss:  1.4902009548239719
Iteration:  4400        Average Loss:  1.5684896112601456
Iteration:  4500        Average Loss:  1.7609827935806988
Iteration:  4600        Average Loss:  1.5815145623631524
Iteration:  4700        Average Loss:  1.565350446112498
Iteration:  4800        Average Loss:  1.5516081787971536
Iteration:  4900        Average Loss:  1.5295322733678338
Iteration:  5000        Average Loss:  1.588473043611208
Accuracy 0.1683991683991684
Iteration:  5100        Average Loss:  1.5550262296966042
Iteration:  5200        Average Loss:  1.4159887144734469
Iteration:  5300        Average Loss:  1.458483691031154
Iteration:  5400        Average Loss:  1.7360231571239957
Iteration:  5500        Average Loss:  1.635207398095435
Iteration:  5600        Average Loss:  1.7149576490941865
Iteration:  5700        Average Loss:  1.5645548954850739
Iteration:  5800        Average Loss:  1.6598686961365223
Iteration:  5900        Average Loss:  1.5532032854353273
Iteration:  6000        Average Loss:  1.4924545921995733
Iteration:  6100        Average Loss:  1.6574982084162406
Iteration:  6200        Average Loss:  1.6513919620024475
Iteration:  6300        Average Loss:  1.561225591654942
Iteration:  6400        Average Loss:  1.52346097836119
Iteration:  6500        Average Loss:  1.5445979731549735
Iteration:  6600        Average Loss:  1.5310865678163637
Iteration:  6700        Average Loss:  1.7178045095613654
Iteration:  6800        Average Loss:  1.651397985112966
Iteration:  6900        Average Loss:  1.5034864502867782
Iteration:  7000        Average Loss:  1.7127802409847654
Iteration:  7100        Average Loss:  1.454596728348802
Iteration:  7200        Average Loss:  1.5612798182938892
Iteration:  7300        Average Loss:  1.4201793977102386
Iteration:  7400        Average Loss:  1.4962062012969033
Iteration:  7500        Average Loss:  1.5807243027705726
Iteration:  7600        Average Loss:  1.4378354244263347
Iteration:  7700        Average Loss:  1.4652559669447494
Iteration:  7800        Average Loss:  1.8227797116191258
Iteration:  7900        Average Loss:  1.7014122437127188
Iteration:  8000        Average Loss:  1.7119794921767966
Iteration:  8100        Average Loss:  1.439755828878552
Iteration:  8200        Average Loss:  1.6851443705817888
Iteration:  8300        Average Loss:  1.560173773762318
Iteration:  8400        Average Loss:  1.5930018641690697
Iteration:  8500        Average Loss:  1.5291350581901102
Iteration:  8600        Average Loss:  1.7459858603145486
Iteration:  8700        Average Loss:  1.573695175781096
Iteration:  8800        Average Loss:  1.4190371378897058
Iteration:  8900        Average Loss:  1.677350910352686
Iteration:  9000        Average Loss:  1.545384030711721
Iteration:  9100        Average Loss:  1.629619048815669
Iteration:  9200        Average Loss:  1.4577567590241205
Iteration:  9300        Average Loss:  1.4466159939733187
Iteration:  9400        Average Loss:  1.5940502007375796
Iteration:  9500        Average Loss:  1.410378550846024
Iteration:  9600        Average Loss:  1.4454315754066251
Iteration:  9700        Average Loss:  1.651481501122288
Iteration:  9800        Average Loss:  1.6808047887402993
Iteration:  9900        Average Loss:  1.9327163210009775
Writing snapshot to model_iter_10000.mdl
Iteration:  10000       Average Loss:  1.7881168307295991
Accuracy 0.1704781704781705
Iteration:  10100       Average Loss:  1.6683593421945355
Iteration:  10200       Average Loss:  1.7120154974826134
Iteration:  10300       Average Loss:  1.312930660150211
Iteration:  10400       Average Loss:  1.6043997887755794
Iteration:  10500       Average Loss:  1.6239635005795807
Iteration:  10600       Average Loss:  1.5460416503000958
Iteration:  10700       Average Loss:  1.5114255554520684
Iteration:  10800       Average Loss:  1.5393115179236805
Iteration:  10900       Average Loss:  1.4778304045999053
Iteration:  11000       Average Loss:  1.7294820936714606
Iteration:  11100       Average Loss:  1.4889463653076316
Iteration:  11200       Average Loss:  1.736827288790063
Iteration:  11300       Average Loss:  1.6516076047962058
Iteration:  11400       Average Loss:  1.686209498580646
Iteration:  11500       Average Loss:  1.5849161580413331
Iteration:  11600       Average Loss:  1.5985154574816423
Iteration:  11700       Average Loss:  1.633130133968427
Iteration:  11800       Average Loss:  1.60848187176708
Iteration:  11900       Average Loss:  1.8043818510914478
Iteration:  12000       Average Loss:  1.6338757370701384
Iteration:  12100       Average Loss:  1.5604451339201455
Iteration:  12200       Average Loss:  1.565414062224821
Iteration:  12300       Average Loss:  1.5809776393077575
Iteration:  12400       Average Loss:  1.4524325791314865
Iteration:  12500       Average Loss:  1.7241071997838813
Iteration:  12600       Average Loss:  1.5250385922863459
Iteration:  12700       Average Loss:  1.4444856014399412
Iteration:  12800       Average Loss:  1.5625462762298934
Iteration:  12900       Average Loss:  1.4476777698472694
Iteration:  13000       Average Loss:  1.576366641861139
Iteration:  13100       Average Loss:  1.4767355212330568
Iteration:  13200       Average Loss:  1.3676681063984342
Iteration:  13300       Average Loss:  1.6517626436937476
Iteration:  13400       Average Loss:  1.3542420399423158
Iteration:  13500       Average Loss:  1.5088105555363012
Iteration:  13600       Average Loss:  1.4029277886750575
Iteration:  13700       Average Loss:  1.8616840530850252
Iteration:  13800       Average Loss:  1.6098923934161191
Iteration:  13900       Average Loss:  1.6699799532088404
Iteration:  14000       Average Loss:  1.6304594570435984
Iteration:  14100       Average Loss:  1.5952966682893506
Iteration:  14200       Average Loss:  1.479887508637325
Iteration:  14300       Average Loss:  1.5595819087703129
Iteration:  14400       Average Loss:  1.453976013158271
Iteration:  14500       Average Loss:  1.3445553707291749
Iteration:  14600       Average Loss:  1.4525398280276767
Iteration:  14700       Average Loss:  1.432801537972769
Iteration:  14800       Average Loss:  1.303851657237913
Iteration:  14900       Average Loss:  1.555932501572821
Iteration:  15000       Average Loss:  1.371752186643388
Accuracy 0.1683991683991684
Iteration:  15100       Average Loss:  1.5531730573658025
Iteration:  15200       Average Loss:  1.470469005325111
Iteration:  15300       Average Loss:  1.5078799889558832
Iteration:  15400       Average Loss:  1.4712130176489762
Iteration:  15500       Average Loss:  1.490004965692548
Iteration:  15600       Average Loss:  1.5399133937291805
Iteration:  15700       Average Loss:  1.4021450874975896
Iteration:  15800       Average Loss:  1.4904908259440777
Iteration:  15900       Average Loss:  1.512191800573621
Iteration:  16000       Average Loss:  1.558911891027943
Iteration:  16100       Average Loss:  1.3936090738719218
Iteration:  16200       Average Loss:  1.5340067350769946
Iteration:  16300       Average Loss:  1.4126789506233444
Iteration:  16400       Average Loss:  1.6240713227501293
Iteration:  16500       Average Loss:  1.3729227153161405
Iteration:  16600       Average Loss:  1.4709508138329082
Iteration:  16700       Average Loss:  1.4536482376844164
Iteration:  16800       Average Loss:  1.653808330640999
Iteration:  16900       Average Loss:  1.4797993131620195
Iteration:  17000       Average Loss:  1.511241229951745
Iteration:  17100       Average Loss:  1.569180332037285
Iteration:  17200       Average Loss:  1.4440351625677363
Iteration:  17300       Average Loss:  1.3084658283845831
Iteration:  17400       Average Loss:  1.417404439529927
Iteration:  17500       Average Loss:  1.6171396307200485
Iteration:  17600       Average Loss:  1.5003063320418477
Iteration:  17700       Average Loss:  1.4477993093918937
Iteration:  17800       Average Loss:  1.4867595233964856
Iteration:  17900       Average Loss:  1.5197750507289671
Iteration:  18000       Average Loss:  1.357035026568023
Iteration:  18100       Average Loss:  1.5428006878856269
Iteration:  18200       Average Loss:  1.5020966881342823
Iteration:  18300       Average Loss:  1.582236512591844
Iteration:  18400       Average Loss:  1.4824426537449638
Iteration:  18500       Average Loss:  1.407202555319728
Iteration:  18600       Average Loss:  1.4184832753778636
Iteration:  18700       Average Loss:  1.4556695900174017
Iteration:  18800       Average Loss:  1.3630070137630435
Iteration:  18900       Average Loss:  1.3894940282396824
Iteration:  19000       Average Loss:  1.5233518228565472
Iteration:  19100       Average Loss:  1.3368589339407597
Iteration:  19200       Average Loss:  1.3830644287573322
Iteration:  19300       Average Loss:  1.6043624237452179
Iteration:  19400       Average Loss:  1.3658667166203917
Iteration:  19500       Average Loss:  1.511619566426951
Iteration:  19600       Average Loss:  1.48619490854161
Iteration:  19700       Average Loss:  1.577364284693398
Iteration:  19800       Average Loss:  1.2115136491298808
Iteration:  19900       Average Loss:  1.555881956544721
Writing snapshot to model_iter_20000.mdl
Iteration:  20000       Average Loss:  1.5458207182765493
Accuracy 0.16632016632016633
Iteration:  20100       Average Loss:  1.4160631065608655
Iteration:  20200       Average Loss:  1.4128790810258351
Iteration:  20300       Average Loss:  1.5057277963918059
Iteration:  20400       Average Loss:  1.2987530029013465
Iteration:  20500       Average Loss:  1.4779724060760773
Iteration:  20600       Average Loss:  1.6198671727273148
Iteration:  20700       Average Loss:  1.5057826506444767
Iteration:  20800       Average Loss:  1.4252897233153718
Iteration:  20900       Average Loss:  1.5423961313911676
Iteration:  21000       Average Loss:  1.5132558071659497
Iteration:  21100       Average Loss:  1.3718846024181255
Iteration:  21200       Average Loss:  1.4474190792925634
Iteration:  21300       Average Loss:  1.4907241279645842
Iteration:  21400       Average Loss:  1.475817208614356
Iteration:  21500       Average Loss:  1.58934717604508
Iteration:  21600       Average Loss:  1.4180447342363336
Iteration:  21700       Average Loss:  1.4636535756339806
Iteration:  21800       Average Loss:  1.4346432891313905
Iteration:  21900       Average Loss:  1.4602171541082516
Iteration:  22000       Average Loss:  1.4492915304654699
Iteration:  22100       Average Loss:  1.4675312485734793
Iteration:  22200       Average Loss:  1.4791703967361982
Iteration:  22300       Average Loss:  1.3255439980712829
Iteration:  22400       Average Loss:  1.3712476660759625
Iteration:  22500       Average Loss:  1.2499051328774364
Iteration:  22600       Average Loss:  1.3829731484146865
Iteration:  22700       Average Loss:  1.3005676406669
Iteration:  22800       Average Loss:  1.4668481172293741
Iteration:  22900       Average Loss:  1.4026473645364
Iteration:  23000       Average Loss:  1.3761769936644364
Iteration:  23100       Average Loss:  1.3492605149162296
Iteration:  23200       Average Loss:  1.5275329480257187
Iteration:  23300       Average Loss:  1.5225145028632192
Iteration:  23400       Average Loss:  1.4656079024169608
Iteration:  23500       Average Loss:  1.461356210441571
Iteration:  23600       Average Loss:  1.3501108149782846
Iteration:  23700       Average Loss:  1.3382166301003724
Iteration:  23800       Average Loss:  1.5388944399853952
Iteration:  23900       Average Loss:  1.4264109667258267
Iteration:  24000       Average Loss:  1.5171940876593544
Iteration:  24100       Average Loss:  1.3178456753858903
Iteration:  24200       Average Loss:  1.3692039385597627
Iteration:  24300       Average Loss:  1.3461060967043137
Iteration:  24400       Average Loss:  1.4600061536790847
Iteration:  24500       Average Loss:  1.3852809179557746
Iteration:  24600       Average Loss:  1.378921653418338
Iteration:  24700       Average Loss:  1.7189148776190182
Iteration:  24800       Average Loss:  1.3065007874990502
Iteration:  24900       Average Loss:  1.346420979627723
Iteration:  25000       Average Loss:  1.517437997004462
Accuracy 0.17463617463617465
Iteration:  25100       Average Loss:  1.2941661529924742
Iteration:  25200       Average Loss:  1.3362337848303094
Iteration:  25300       Average Loss:  1.473512796044864
Iteration:  25400       Average Loss:  1.3749653958916825
Iteration:  25500       Average Loss:  1.4118186745058818
Iteration:  25600       Average Loss:  1.2358342878616089
Iteration:  25700       Average Loss:  1.2080128771925323
Iteration:  25800       Average Loss:  1.389115774267773
Iteration:  25900       Average Loss:  1.3256913547645706
Iteration:  26000       Average Loss:  1.4815901174858124
Iteration:  26100       Average Loss:  1.3346628274963044
Iteration:  26200       Average Loss:  1.1804243717467033
Iteration:  26300       Average Loss:  1.4172890982018276
Iteration:  26400       Average Loss:  1.2252598197151277
Iteration:  26500       Average Loss:  1.3688872435284634
Iteration:  26600       Average Loss:  1.392841627391911
Iteration:  26700       Average Loss:  1.0901972071836246
Iteration:  26800       Average Loss:  1.337366009953968
Iteration:  26900       Average Loss:  1.4054092381518433
Iteration:  27000       Average Loss:  1.3953729380092847
Iteration:  27100       Average Loss:  1.3020717006048508
Iteration:  27200       Average Loss:  1.3851478685343455
Iteration:  27300       Average Loss:  1.2436076264407914
Iteration:  27400       Average Loss:  1.3205628312152857
Iteration:  27500       Average Loss:  1.3089161993889715
Iteration:  27600       Average Loss:  1.3687940785183037
Iteration:  27700       Average Loss:  1.35471788668858
Iteration:  27800       Average Loss:  1.4189825611995526
Iteration:  27900       Average Loss:  1.3351579077800884
Iteration:  28000       Average Loss:  1.3985418967096903
Iteration:  28100       Average Loss:  1.3639824006602068
Iteration:  28200       Average Loss:  1.3448646839445602
Iteration:  28300       Average Loss:  1.2555441818706603
Iteration:  28400       Average Loss:  1.2570186182157124
Iteration:  28500       Average Loss:  1.3569999353649354
Iteration:  28600       Average Loss:  1.3589786198359182
Iteration:  28700       Average Loss:  1.4705658434365976
Iteration:  28800       Average Loss:  1.2210715888427996
Iteration:  28900       Average Loss:  1.3606510608275064
Iteration:  29000       Average Loss:  1.3731529843078008
Iteration:  29100       Average Loss:  1.6640060186650922
Iteration:  29200       Average Loss:  1.4367129511171575
Iteration:  29300       Average Loss:  1.5672103760204377
Iteration:  29400       Average Loss:  1.3378312049409582
Iteration:  29500       Average Loss:  1.5857497782743741
Iteration:  29600       Average Loss:  1.3469221630860764
Iteration:  29700       Average Loss:  1.3054370300684133
Iteration:  29800       Average Loss:  1.3092169328285623
Iteration:  29900       Average Loss:  1.2882237854465854
Writing snapshot to model_iter_30000.mdl
Iteration:  30000       Average Loss:  1.387436584205105
Accuracy 0.17255717255717257
Iteration:  30100       Average Loss:  1.382448709835392
Iteration:  30200       Average Loss:  1.4745129858907036
Iteration:  30300       Average Loss:  1.1888981612398781
Iteration:  30400       Average Loss:  1.5783888139682845
Iteration:  30500       Average Loss:  1.4216768898131904
Iteration:  30600       Average Loss:  1.3595968785317325
Iteration:  30700       Average Loss:  1.1969574878617089
Iteration:  30800       Average Loss:  1.0399857218493296
Iteration:  30900       Average Loss:  1.4010842532061847
Iteration:  31000       Average Loss:  1.3498418524628395
Iteration:  31100       Average Loss:  1.2106645638951377
Iteration:  31200       Average Loss:  1.3361634415415986
Iteration:  31300       Average Loss:  1.4461239813113855
Iteration:  31400       Average Loss:  1.3649807283034294
Iteration:  31500       Average Loss:  1.2372007034382901
Iteration:  31600       Average Loss:  1.5910161663450644
Iteration:  31700       Average Loss:  1.3899250364376998
Iteration:  31800       Average Loss:  1.4602403758266298
Iteration:  31900       Average Loss:  1.1975468896989596
Iteration:  32000       Average Loss:  1.2764188404939836
Iteration:  32100       Average Loss:  1.3968119226293703
Iteration:  32200       Average Loss:  1.4245241282565866
Iteration:  32300       Average Loss:  1.3971041221651468
Iteration:  32400       Average Loss:  1.2797858874506867
Iteration:  32500       Average Loss:  1.2883753308524195
Iteration:  32600       Average Loss:  1.119908659180445
Iteration:  32700       Average Loss:  1.3914404580263926
Iteration:  32800       Average Loss:  1.418410350455967
Iteration:  32900       Average Loss:  1.4319862347305266
Iteration:  33000       Average Loss:  1.462055919625098
Iteration:  33100       Average Loss:  1.5277773241687997
Iteration:  33200       Average Loss:  1.3823611712191992
Iteration:  33300       Average Loss:  1.2681898914976575
Iteration:  33400       Average Loss:  1.1594382514905714
Iteration:  33500       Average Loss:  1.2087697332355676
Iteration:  33600       Average Loss:  1.346991779571007
Iteration:  33700       Average Loss:  1.2856603277084533
Iteration:  33800       Average Loss:  1.293886695948352
Iteration:  33900       Average Loss:  1.3128717918456534
Iteration:  34000       Average Loss:  1.429794829450791
Iteration:  34100       Average Loss:  1.4005262756855945
Iteration:  34200       Average Loss:  1.329954053696592
Iteration:  34300       Average Loss:  1.3735691153647756
Iteration:  34400       Average Loss:  1.1745354904864362
Iteration:  34500       Average Loss:  1.2385428094659867
Iteration:  34600       Average Loss:  1.2627999367279157
Iteration:  34700       Average Loss:  1.464418519764564
Iteration:  34800       Average Loss:  1.5271287971774776
Iteration:  34900       Average Loss:  1.331034415963393
Iteration:  35000       Average Loss:  1.4725113436555874
Accuracy 0.18087318087318088