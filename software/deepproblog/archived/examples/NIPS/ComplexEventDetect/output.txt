Training for 1 epochs (23951 iterations).
/home/nothing/miniconda3/envs/DPL/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Accuracy 0.14346895074946467
Epoch 1
Iteration:  100         Average Loss:  1.995632213534251
Iteration:  200         Average Loss:  1.9388925048412187
Iteration:  300         Average Loss:  1.8547187949245991
Iteration:  400         Average Loss:  1.748600442231884
Iteration:  500         Average Loss:  1.8529932676785499
Iteration:  600         Average Loss:  1.8703370628174243
Iteration:  700         Average Loss:  1.8076138483468
Iteration:  800         Average Loss:  1.6972717281899758
Iteration:  900         Average Loss:  1.8149027449807846
Iteration:  1000        Average Loss:  1.687025073776074
Iteration:  1100        Average Loss:  1.5943203424714636
Iteration:  1200        Average Loss:  1.7227992827434346
Iteration:  1300        Average Loss:  1.6375064663310803
Iteration:  1400        Average Loss:  1.7495449686243125
Iteration:  1500        Average Loss:  1.6817106336872996
Iteration:  1600        Average Loss:  1.5842507111683384
Iteration:  1700        Average Loss:  1.6271588166936843
Iteration:  1800        Average Loss:  1.6389207573344884
Iteration:  1900        Average Loss:  1.6170472837012833
Iteration:  2000        Average Loss:  1.602720598505421
Iteration:  2100        Average Loss:  1.693494096318546
Iteration:  2200        Average Loss:  1.5588527617904142
Iteration:  2300        Average Loss:  1.524317786530586
Iteration:  2400        Average Loss:  1.5549591328722814
Iteration:  2500        Average Loss:  1.5690511184410452
Iteration:  2600        Average Loss:  1.6027936340439843
Iteration:  2700        Average Loss:  1.7501737464774598
Iteration:  2800        Average Loss:  1.7264111316924506
Iteration:  2900        Average Loss:  1.7027627573414987
Iteration:  3000        Average Loss:  1.569867237780831
Iteration:  3100        Average Loss:  1.5159397132602495
Iteration:  3200        Average Loss:  1.614708005105344
Iteration:  3300        Average Loss:  1.5111722027990129
Iteration:  3400        Average Loss:  1.6375197161498565
Iteration:  3500        Average Loss:  1.6147927275236185
Iteration:  3600        Average Loss:  1.513316167017977
Iteration:  3700        Average Loss:  1.6078522138548763
Iteration:  3800        Average Loss:  1.4945035315686173
Iteration:  3900        Average Loss:  1.571256410538206
Iteration:  4000        Average Loss:  1.7430049623739243
Iteration:  4100        Average Loss:  1.6855610816602904
Iteration:  4200        Average Loss:  1.6505233195623576
Iteration:  4300        Average Loss:  1.596121759621339
Iteration:  4400        Average Loss:  1.6739152932239738
Iteration:  4500        Average Loss:  1.5569649376105756
Iteration:  4600        Average Loss:  1.508887899195848
Iteration:  4700        Average Loss:  1.547471698321604
Iteration:  4800        Average Loss:  1.5177528255166877
Iteration:  4900        Average Loss:  1.514215759509039
Iteration:  5000        Average Loss:  1.6783576467644628
Accuracy 0.14346895074946467
Iteration:  5100        Average Loss:  1.5475958682655
Iteration:  5200        Average Loss:  1.7168431146026135
Iteration:  5300        Average Loss:  1.6546056917066025
Iteration:  5400        Average Loss:  1.549150460064553
Iteration:  5500        Average Loss:  1.467907220029622
Iteration:  5600        Average Loss:  1.5314027105937178
Iteration:  5700        Average Loss:  1.3909040642562067
Iteration:  5800        Average Loss:  1.5175852463376882
Iteration:  5900        Average Loss:  1.4466078682268921
Iteration:  6000        Average Loss:  1.5687242071103444
Iteration:  6100        Average Loss:  1.5571169801024598
Iteration:  6200        Average Loss:  1.4613592874926657
Iteration:  6300        Average Loss:  1.6206763215708515
Iteration:  6400        Average Loss:  1.5588341232894836
Iteration:  6500        Average Loss:  1.6967685669465278
Iteration:  6600        Average Loss:  1.662836963869501
Iteration:  6700        Average Loss:  1.5872337326573833
Iteration:  6800        Average Loss:  1.4406383620006635
Iteration:  6900        Average Loss:  1.5145511393249607
Iteration:  7000        Average Loss:  1.6472994988966894
Iteration:  7100        Average Loss:  1.577962563078249
Iteration:  7200        Average Loss:  1.6083923261217672
Iteration:  7300        Average Loss:  1.4682131610934488
Iteration:  7400        Average Loss:  1.6254374386313017
Iteration:  7500        Average Loss:  1.538497915595125
Iteration:  7600        Average Loss:  1.2849113123001956
Iteration:  7700        Average Loss:  1.372763102802999
Iteration:  7800        Average Loss:  1.4636968332751623
Iteration:  7900        Average Loss:  1.5408675555864173
Iteration:  8000        Average Loss:  1.604091638442911
Iteration:  8100        Average Loss:  1.4347526282706395
Iteration:  8200        Average Loss:  1.5724746146619106
Iteration:  8300        Average Loss:  1.529738157848994
Iteration:  8400        Average Loss:  1.4563508189917682
Iteration:  8500        Average Loss:  1.6519563901334988
Iteration:  8600        Average Loss:  1.4832922242653535
Iteration:  8700        Average Loss:  1.39296026272668
Iteration:  8800        Average Loss:  1.4805645242738819
Iteration:  8900        Average Loss:  1.5109809951597648
Iteration:  9000        Average Loss:  1.4855343045172962
Iteration:  9100        Average Loss:  1.501386675722193
Iteration:  9200        Average Loss:  1.510800380526958
Iteration:  9300        Average Loss:  1.3293831124266644
Iteration:  9400        Average Loss:  1.5624174138794749
Iteration:  9500        Average Loss:  1.5536853494998164
Iteration:  9600        Average Loss:  1.5137856843314819
Iteration:  9700        Average Loss:  1.477538512722657
Iteration:  9800        Average Loss:  1.45808901869906
Iteration:  9900        Average Loss:  1.6434628949718912
Writing snapshot to model_iter_10000.mdl
Iteration:  10000       Average Loss:  1.5032967227826572
Accuracy 0.14346895074946467
Iteration:  10100       Average Loss:  1.382266564028006
Iteration:  10200       Average Loss:  1.414910897564064
Iteration:  10300       Average Loss:  1.4945987641048344
Iteration:  10400       Average Loss:  1.4148334333613213
Iteration:  10500       Average Loss:  1.5041663750444187
Iteration:  10600       Average Loss:  1.3890074490438757
Iteration:  10700       Average Loss:  1.6101785479688382
Iteration:  10800       Average Loss:  1.5801127403930142
Iteration:  10900       Average Loss:  1.5559836706508596
Iteration:  11000       Average Loss:  1.5556604320065655
Iteration:  11100       Average Loss:  1.5511206086115394
Iteration:  11200       Average Loss:  1.5382476783495678
Iteration:  11300       Average Loss:  1.3751469556996903
Iteration:  11400       Average Loss:  1.4042860868907185
Iteration:  11500       Average Loss:  1.577636648252382
Iteration:  11600       Average Loss:  1.5479950005545202
Iteration:  11700       Average Loss:  1.449110141666
Iteration:  11800       Average Loss:  1.2292879459446298
Iteration:  11900       Average Loss:  1.5600263396405012
Iteration:  12000       Average Loss:  1.3897488076948374
Iteration:  12100       Average Loss:  1.301802940293036
Iteration:  12200       Average Loss:  1.4599659462487022
Iteration:  12300       Average Loss:  1.3018556501980547
Iteration:  12400       Average Loss:  1.4117408114106138
Iteration:  12500       Average Loss:  1.4770705374201178
Iteration:  12600       Average Loss:  1.2883048695103922
Iteration:  12700       Average Loss:  1.5754952357898975
Iteration:  12800       Average Loss:  1.4075864044108832
Iteration:  12900       Average Loss:  1.381181519262662
Iteration:  13000       Average Loss:  1.4220415833396816
Iteration:  13100       Average Loss:  1.214113075618714
Iteration:  13200       Average Loss:  1.6134482795636875
Iteration:  13300       Average Loss:  1.3303725429249282
Iteration:  13400       Average Loss:  1.566268228648713
Iteration:  13500       Average Loss:  1.4434641655748175
Iteration:  13600       Average Loss:  1.4590959148255762
Iteration:  13700       Average Loss:  1.3428688320755475
Iteration:  13800       Average Loss:  1.5139104304728281
Iteration:  13900       Average Loss:  1.2808489249470592
Iteration:  14000       Average Loss:  1.4522900165588934
Iteration:  14100       Average Loss:  1.4449694970144054
Iteration:  14200       Average Loss:  1.292357231027385
Iteration:  14300       Average Loss:  1.4718138932867644
Iteration:  14400       Average Loss:  1.4186366578088894
Iteration:  14500       Average Loss:  1.4460791466915885
Iteration:  14600       Average Loss:  1.4082837162178987
Iteration:  14700       Average Loss:  1.6884980944197685
Iteration:  14800       Average Loss:  1.611670683719192
Iteration:  14900       Average Loss:  1.6849822130947083
Iteration:  15000       Average Loss:  1.2852850660569164
Accuracy 0.15417558886509636
Iteration:  15100       Average Loss:  1.3961101008154537
Iteration:  15200       Average Loss:  1.4129407292201366
Iteration:  15300       Average Loss:  1.390746823586295
Iteration:  15400       Average Loss:  1.6465783128789413
Iteration:  15500       Average Loss:  1.450444005459347
Iteration:  15600       Average Loss:  1.2821667347301036
Iteration:  15700       Average Loss:  1.331743035387579
Iteration:  15800       Average Loss:  1.4175747743036138
Iteration:  15900       Average Loss:  1.395556658422047
Iteration:  16000       Average Loss:  1.4088831795976802
Iteration:  16100       Average Loss:  1.5552697117923378
Iteration:  16200       Average Loss:  1.2756795061770543
Iteration:  16300       Average Loss:  1.2929006823436082
Iteration:  16400       Average Loss:  1.477805946815744
Iteration:  16500       Average Loss:  1.5347665906812806
Iteration:  16600       Average Loss:  1.4231201921785563
Iteration:  16700       Average Loss:  1.4180573895511268
Iteration:  16800       Average Loss:  1.3700859851816858
Iteration:  16900       Average Loss:  1.4869918694371815
Iteration:  17000       Average Loss:  1.3834112079588783
Iteration:  17100       Average Loss:  1.4671522252201095
Iteration:  17200       Average Loss:  1.3989368929009753
Iteration:  17300       Average Loss:  1.3369154639444034
Iteration:  17400       Average Loss:  1.513690744498608
Iteration:  17500       Average Loss:  1.376458760522474
Iteration:  17600       Average Loss:  1.475319904453871
Iteration:  17700       Average Loss:  1.35964806016892
Iteration:  17800       Average Loss:  1.3412670594465168
Iteration:  17900       Average Loss:  1.4232360332581429
Iteration:  18000       Average Loss:  1.362266850027337
Iteration:  18100       Average Loss:  1.275125091226723
Iteration:  18200       Average Loss:  1.383472738767248
Iteration:  18300       Average Loss:  1.2615517852105256
Iteration:  18400       Average Loss:  1.4243193827099545
Iteration:  18500       Average Loss:  1.447149046407969
Iteration:  18600       Average Loss:  1.2059690648165988
Iteration:  18700       Average Loss:  1.5044189738257583
Iteration:  18800       Average Loss:  1.3652936050845133
Iteration:  18900       Average Loss:  1.4673079429286418
Iteration:  19000       Average Loss:  1.3790190014506303
Iteration:  19100       Average Loss:  1.4558717631445015
Iteration:  19200       Average Loss:  1.499903837443139
Iteration:  19300       Average Loss:  1.3401785872747325
Iteration:  19400       Average Loss:  1.4148248391888634
Iteration:  19500       Average Loss:  1.272551173939281
Iteration:  19600       Average Loss:  1.4210240458127508
Iteration:  19700       Average Loss:  1.4699981460378695
Iteration:  19800       Average Loss:  1.580391659085207
Iteration:  19900       Average Loss:  1.317795238668282
Writing snapshot to model_iter_20000.mdl
Iteration:  20000       Average Loss:  1.439109178006086
Accuracy 0.14989293361884368